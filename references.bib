@misc{anglimAnswerImageFull2010,
  title = {Answer to ``{{Image}} on Full Slide in Beamer Package''},
  author = {Anglim, Jeromy},
  year = {2010},
  month = oct,
  journal = {TeX - LaTeX Stack Exchange},
  urldate = {2024-01-22},
}

@misc{fiandrinoAnswerDesignCustom2013,
  title = {Answer to ``{{Design}} a Custom {{Beamer}} Theme from Scratch''},
  author = {Fiandrino, Claudio},
  year = {2013},
  month = nov,
  journal = {TeX - LaTeX Stack Exchange},
  urldate = {2024-01-22},
}

@misc{kormyloAnswerMakeTikz2017,
  title = {Answer to ``{{Make}} Tikz Nodes Flush with Slide Edges in Beamer Title Slide''},
  author = {Kormylo, John},
  year = {2017},
  month = oct,
  journal = {TeX - LaTeX Stack Exchange},
  urldate = {2024-01-22}
}

@misc{rpiHomeStrategicCommunications,
  title = {Home | {{Strategic Communications}} and {{External Relations}}},
  author = {RPI},
  urldate = {2024-01-22},
  howpublished = {https://scer.rpi.edu/},
}

@misc{samcarter_is_at_topanswers.xyzAnswerBeamerVertical2013,
  title = {Answer to "{{Beamer}}: {{Vertical}} Alignment of Multi-Column {{ToC}}"},
  shorttitle = {Answer to "{{Beamer}}},
  author = {{samcarter\_is\_at\_topanswers.xyz}},
  year = {2013},
  month = oct,
  journal = {TeX - LaTeX Stack Exchange},
  urldate = {2024-01-22},
}

@misc{scharrerAnswerMinipageColumns2011,
  title = {Answer to "{{Minipage}}, Columns, Tabular: Differences in Applicability?"},
  shorttitle = {Answer to "{{Minipage}}, Columns, Tabular},
  author = {Scharrer, Martin},
  year = {2011},
  month = nov,
  journal = {TeX - LaTeX Stack Exchange},
  urldate = {2024-01-22},
}

@misc{campaAnswerAtbeginsectionReturns2022,
  title = {Answer to "{{Atbeginsection}} Returns Missing Number Treated as Zero"},
  author = {{campa}},
  year = {2022},
  month = sep,
  journal = {TeX - LaTeX Stack Exchange},
  urldate = {2024-01-22},
}

@inproceedings{cazenavetteDatasetDistillationMatching2022,
  title = {Dataset {{Distillation}} by {{Matching Training Trajectories}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cazenavette, George and Wang, Tongzhou and Torralba, Antonio and Efros, Alexei A. and Zhu, Jun-Yan},
  year = {2022},
  month = jun,
  pages = {10708--10717},
  issn = {2575-7075},
  doi = {10.1109/CVPR52688.2022.01045},
  abstract = {Dataset distillation is the task of synthesizing a small dataset such that a model trained on the synthetic set will match the test accuracy of the model trained on the full dataset. In this paper, we propose a new formulation that optimizes our distilled data to guide networks to a similar state as those trained on real data across many training steps. Given a network, we train it for several iterations on our distilled data and optimize the distilled data with respect to the distance between the synthetically trained parameters and the parameters trained on real data. To efficiently obtain the initial and target network parameters for large-scale datasets, we pre-compute and store training trajectories of expert networks trained on the real dataset. Our method handily outperforms existing methods and also allows us to distill higher-resolution visual data.},
}
